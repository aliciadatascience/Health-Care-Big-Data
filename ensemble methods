Gradient Desent
    Gradient descent is a basic optimization method that has widely used in ML.

Stochastic Gradient Descent Method:
    SGD is a variant of gradient descent for handing big data set. In traditional Gradient Descent Methods, we have to compute the likelihooe of the entire dataset, then compute the gradient of the entire dataset, then repeat many times.
    SGD compute the likelihood function and the gradient on a random subset of data points.

Bias and Variance
    1. Bias refers to the prediction error due to the wrong modeling assumption.
    2. Variance refers to the error from the sensitivity to small fluctuation in the training dataset. If high variance means more scattered.

  
